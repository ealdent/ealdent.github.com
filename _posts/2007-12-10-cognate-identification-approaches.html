---
layout: post
title: "Cognate Identification:  Approaches"
tags: ["cognate identification", "cognate identification", "computational linguistics", "computational linguistics", "historical linguistics", "historical linguistics", "language change", "language change", "linguistics", "linguistics", "natural language processing", "natural language processing", "phonology", "phonology"]
---
<hr /><br />Original post can be found at:  <a href="http://ealdent.wordpress.com/2007/12/10/cognate-identification-approaches/" target="_blank">http://ealdent.wordpress.com/2007/12/10/cognate-identification-approaches/</a><br /><br />
<p align="justify">In my previous post on cognate identification, I gave two definitions for cognates:  strict and loose (orthographic).  Strict cognates are words in two related languages that descended from the same word in the ancestor language.  Loose cognates are words in two languages that are spelled or pronounced similarly (depending on the data consists of phonetic transcriptions or plain text).  These two definitions help form the basis for how I choose to classify approaches to doing cognate identification, but the source of data is the bigger factor, in my opinion.  The <strong>orthographic approach</strong> looks at plain text and attempts to do some sort of string matching or statistical correlation based on the written (typeset) characters of the language.  The <strong>phonetic approach</strong> relies on phonetic transcriptions of words in the language.  Phonetic transcriptions are usually done in the <a href="http://en.wikipedia.org/wiki/Ipa" target="_blank">International Phonetic Alphabet</a> (IPA) but any standard form of representing sounds will work.  One such example is the <a href="ftp://ftp.cs.cmu.edu/afs/cs.cmu.edu/data/anonftp/project/fgdata/dict/" target="_blank">Carnegie Mellon Pronouncing Dictionary</a>.  Phonetic approaches may use string matching techniques, but there are also a number of inductive methods based on phonology that have been tried to good effect.</p>
<p align="justify">So a good question might be why does the data being used matter so much to these techniques?  Why not classify the two approaches as to whether they look for loose or strict cognates?  Might there not be another way of classifying the approaches to cognate identification beyond these two?<span>  </span>Or is there an entirely different set of classes that would better describe them?  To answer the last two questions, I will say that there very well may be better ways of classifying these algorithms.  As <a href="http://anileklavya.wordpress.com/" target="_blank">Anil</a> pointed out in the comments to my last post, the two definitions lend themselves to different applications.  From the papers that I read, it seemed that when researchers looked at plain text data, there was a completely different mindset than in papers where researchers used phonetic transcriptions.  For the former, the goal was usually finding translational equivalences in bitext and for the latter the goal is more as an aid to linguists attempting to reconstruct dead languages or establish relationships between languages.</p>
<p align="justify">With plain text, it is very difficult to infer sound correspondences between two languages.  In Old English, the orthography developed by scribes corresponded directly to the spoken form.  As English changed over the 1000+ years since then, the orthographic forms of words have frozen in some cases and not in others.  For example, the word <em>knight</em> was originally spelled <em>cniht </em>and the <em>c </em>and <em>h</em> were both pronounced.  The divergence of orthographic and phonetic forms can result in any number of problems and so it influences the ways of thinking about the task.  On the other hand, phonetic approaches suffer due to data scarcity.  Obtaining phonetic transcriptions is expensive as it requires the effort of linguists or individuals with specific, extensive training in the area.  There are ways of obtaining phonetic transcriptions automatically, but these methods are not perfect and so result in noisy data, making this data practically useless for historical linguists.</p>
<p align="justify">In my next post, I will go into orthographic approaches in more detail, describing some of the papers I looked at and the methods they used.  After that, I will begin discussing phonetic approaches, which are more numerous.  I will also begin to look at how machine learning is being used to tackle cognate identification.</p>
<p align="justify">View all posts on <a href="http://mendicantbug.com/category/cognate-identification/">cognate identification</a>.</p>
