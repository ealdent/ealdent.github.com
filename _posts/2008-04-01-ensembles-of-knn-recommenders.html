---
layout: post
title: "Ensembles of kNN Recommenders"
tags: ["cmu", "cmu", "ensemble methods", "ensemble methods", "ensembles", "ensembles", "information retrieval", "information retrieval", "kdd", "kdd", "kdd cup", "kdd cup", "knn", "knn", "machine learning", "machine learning", "netflix prize", "netflix prize", "recommender systems", "recommender systems", "rmse", "rmse"]
---
<p align="justify">I've been messing around with recommender systems for the past year and a half, but not using the kNN (k-Nearest Neighbors) algorithm.  However, my current homework assignment for my Information Retrieval class is to implement kNN for a subset of the <a href="http://netflixprize.com" target="_blank">Netflix Prize</a> data.  The data we are working with is about 800k ratings, which is slightly smaller than the <a href="http://grouplens.org/node/73" target="_blank">MovieLens</a> dataset, which was the previous dataset of choice for research on movie recommender systems.  The entire Netflix data set dwarfs the MovieLens set by a factor of 100, so it is quickly replacing MovieLens in papers.  The Netflix data is much sparser than the MovieLens, which changes things, as well.</p>
<p align="justify"><a href="http://en.wikipedia.org/wiki/KNN" target="_blank">kNN</a> is a fairly simple machine learning algorithm to implement.  On a dataset the size of Netflix, it's still easy to do stupid things that cause it to take forever.  Recommender systems typically match users to movies on a scale, which is the user's rating for that item.  In the case of Netflix, the scale is 1 (hate) to 5 (love).  For the Netflix Prize, the goal is to guess user's ratings on a hidden set as correctly as possible (according to <a href="http://en.wikipedia.org/wiki/Root_mean_squared_error" target="_blank">root mean squared error</a> (RMSE)).  One way of approaching the problem is to create a user-items matrix where the rows correspond to a user, the columns to an item (movie) and the value in each cell is the user's rating for that item.  If the user has not rated the item, it is assigned a zero.  Now, we can split this matrix up into vectors, where each row vector represents a user.  kNN seeks to find similar users to other users (or similar movies to other movies) according to some metric over these vectors.  I won't bother going into the metrics in detail, but they include cosine similarity, Euclidean distance, and Pearson correlation.  The ratings from the chosen <i>k</i> users are combined (either by simply averaging or using some weighted average) to form the prediction for a movie the user has not yet rated.</p>
<p align="justify">So on the test dataset for this assignment, I built three models that had the following RMSE scores:</p>
<p align="center"><table align="center" border="1" cellpadding="3">
<tr>
<td>Model 1</td>
<td>0.9831</td>
</tr>
<tr>
<td>Model 2</td>
<td>1.0371</td>
</tr>
<tr>
<td>Model 3</td>
<td>0.9768</td>
</tr>
</table></p>
<p align="justify">Just guessing the average rating for each movie gives an RMSE of 1.0 in this case, so Models 1 and 3 improve over the baseline, while Model 2 does worse.  The best performing teams in the Netflix prize use ensemble methods to combine various models.  The simple way to do this is just with a linear combination.  So given models {m1, m2, m3} and weights {w1, w2, w3}, the ensemble prediction would be w1m1 + w2m2 + w3m3 (where w1+w2+w3=1.0).  This was the first time I had tried ensembles with recommender systems as well, so imagine my surprise when I hit 0.9469 RMSE with my best choice of w's.  Of course, this is nowhere near the number needed to actually claim the prize, but it was a nice demonstration of the power of ensemble methods.  I recommend checking out the proceedings of last year's <a href="http://www.cs.uic.edu/~liub/KDD-cup-2007/proceedings.html" target="_blank">KDD Cup</a> if you're interested.</p>
