---
layout: post
title: "Is presence really better than frequency?"
tags: ["computational linguistics", "computational linguistics", "machine learning", "machine learning", "opinion mining", "opinion mining", "reproducibility", "reproducibility", "sentiment analysis", "sentiment analysis", "svms", "svms"]
---
<hr /><br />Original post can be found at:  <a href="http://ealdent.wordpress.com/2008/09/30/is-presence-better-than-frequency/" target="_blank">http://ealdent.wordpress.com/2008/09/30/is-presence-better-than-frequency/</a><br /><br />
<p>In my previous post about <a href="http://mendicantbug.com/2008/09/16/sentiment-polarity/" target="_self">sentiment polarity</a>, I talked about results from Pang et al (2002).  One of the conclusions in that paper was that the presence of sentiment words led to better classification results than the frequency of words.  In my experiment in that post, I used tf-idf, a frequency-based measure.  I ran some additional experiments a few days ago when I woke up way too early using presence (binary) weights.  The result was a slight improvement over tf-idf:  86.1% versus 85.7%.  If we ignore document frequency and just use term frequency, the results were terrible:  about 76%.  So presence versus term frequency is much better, but presence versus tf-idf isn't much better.</p>
<p>Or is it?  Even more experiments with tf-idf produced an accuracy of 86.8%.  All of this is based on 10-fold cross validation using the Pang and Lee (2004) data set, just so we're clear.  This seems to contradict their results. Of course, I wasn't able to reproduce their results identically, even though I am using the folds exactly as they described.  This may be due to a pre-processing step I am skipping (or doing extra).  They mention length-normalizing the vectors, which I don't usually bother with.  It's an oft-suggested thing to do with svms, but I have yet to have it actually help me.</p>
<p>So I tried normalizing.  It hurt results for tf-idf, dropping it to 86.6%.  It made no difference for presence, which stayed at 86.1%.  No surprises there.</p>
<p>My results contradict Pang et al (2002) in that tf-idf (frequency-based) out-performs presence.  If I made a mistake, where was it?  I wish their source code were made available.  I guess I could always ask. There is usually some voodoo involved that isn't obvious (to me) in the paper.  This is a-whole-nother topic, <a href="http://www.d.umn.edu/~tpederse/Pubs/pedersen-last-word-2008.pdf" target="_blank">one discussed with far more eloquence</a> (pdf warning) by Ted Pedersen in the latest issue of <em>Computational Linguistics</em>.</p>

<h3>References</h3>
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.  "Thumbs Up?  Sentiment Classification Using Machine Learning Techniques."  In <em>Proceedings of the ACL 02 conference on Empirical Methods in Natural Language Processing - Volume 10</em>, July 2002. [<a href="http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf" target="_blank">pdf</a>]

Bo Pang and Lillian Lee.  "A Sentimental Education:  Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts."  In <em>Proceedings of the ACL, </em>2004. [<a href="http://www.cs.cornell.edu/home/llee/papers/cutsent.pdf" target="_blank">pdf</a>]
