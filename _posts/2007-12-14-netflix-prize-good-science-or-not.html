---
layout: post
title: "Netflix Prize:  Good science or not?"
tags: ["computer science", "computer science", "data mining", "data mining", "machine learning", "machine learning", "netflix", "netflix", "netflix prize", "netflix prize", "ratings", "ratings", "recommendations", "recommendations", "recommender systems", "recommender systems", "research", "research"]
---
<hr>Original post can be found at:  <a href="http://ealdent.wordpress.com/2007/12/14/netflix-prize-good-science-or-not/" target="_blank">http://ealdent.wordpress.com/2007/12/14/netflix-prize-good-science-or-not/</a><br /><br />
<p align="justify"><a href="http://glinden.blogspot.com/2007/12/bellkor-ensemble-for-recommendations.html" target="_blank">Greg Linden</a> and <a href="http://www.daniel-lemire.com/blog/archives/2007/12/13/netflix-an-interesting-machine-learning-game-but-is-it-good-science/" target="_blank">Daniel Lemire</a> have both written a little about the Netflix Prize and whether the systems that are doing the best are really worth anything.  The KorBell system that recently won the Progress Prize consists of 107 different parts in an ensemble system (<em>Note</em>:  the <a href="http://www.research.att.com/~volinsky/netflix/" target="_blank">team</a> of Bob Bell and Yehuda Koren at AT&amp;T goes by BellKor and KorBell on the <a href="http://netflixprize.com/leaderboard" target="_blank">Netflix leaderboard</a>).  The paper is interesting for two reasons:  the ensemble method being used and the fact that only about 3 or 4 of those components are doing the heavy lifting.  Actually, I have no idea whether the actual ensemble algorithm they use would be especially interesting to anyone else, but as I have no experience with ensembles in this context, it was interesting to me.<!--more--></p>
<p align="justify">Daniel Lemire made the point that RMSE (root mean squared error), the metric used for the Netflix prize, isn't particularly useful.  <a href="http://en.wikipedia.org/wiki/Root_mean_squared_error" target="_blank">RMSE</a> is a measure of the error that is computed as follows:</p>
<p><img src="http://ealdent.files.wordpress.com/2007/12/rmse.png" alt="RMSE" border="0" />,</p>
<p align="justify">where x1 and x2 are the actual and predicted movie ratings.  RMSE penalizes larger errors more.  Competitors predict ratings for a set of about 2 million movies that Netflix knows the rating for but has kept secret.  The score that is reported on the leaderboard is for one part of the set of 2 million movies and there is another set whose score is kept secret and is used as a validation set to settle tie-breakers or choose between in the case of multiple candidates for victory.</p>
<p align="justify">So RMSE and other similar metrics (like mean absolute error) are measuring how closely the algorithm can guess the rating some user has already given the movie.  It does not evaluate things like how useful offering that recommendation to the user would be or the novelty of the prediction (will the recommendation surprise and delight the user?).  Also, it doesn't explicitly evaluate how quickly these algorithms learn ratings  (how many ratings does the system need to see to make a good prediction?).</p>
<p align="justify">Then there is the question of whether predicting ratings beyond a certain level of accuracy is at all possible.  I think that it's not.  It may be possible to get the predictions on the Netflix set down to the winning number, but I think there is a point for general applications that just can't be broken and that we are already pretty much there.  This idea is not new (Herlocker et al., 2004).</p>
<p align="justify">To better relate this idea, let's consider the data collection method for recommender systems, with Netflix in particular.  The user begins by having a list of movies presented to her.  She rates the movies she has seen and skips the others.  The system begins learning a profile and presents new movies to be rated.  These tend to be popular movies at first.  Once a profile has been constructed, less popular movies that match the profile are presented and eventually the user has rated a lot of movies they have seen.  What about movies the user has seen but the system never presented?  The user may search for those movies in order to rate them, but I suspect most people aren't that dedicated.  Even with dedication, remembering every movie you have seen is hard (at least it was for me).  This results in a slight skew in the data where the average rating is closer to 4 (on a 5-rating scale) than the average rating for the rating scale (3 in this case).  So let's say our hypothetical user spent an hour or two rating 500 movies.</p>
<p align="justify">What would happen if she came back a month later and rated 100 more?  Would her ratings behave the same way?  Maybe she was feeling optimistic when she first rated the movies and light-hearted movies were remembered more fondly.  Now she has dumped her scumbag of a boyfriend and she's hating the world.  Light-hearted movies are crap!  These 100 new movies are going to have some variance compared to the 500 movies rated before.  The fact is that people generally don't consistently rate items.  This plays hell with recommender systems since they try to find consistencies and predict ratings based on them.</p>
<p align="justify">The point of this discussion has been to illustrate the point that natural user variance may prevent systems from improving beyond a certain point using metrics like RMSE and mean absolute error.  Going back to the Netflix issue, this means that improving rating prediction accuracy by 10% is trying to break this recommender system equivalent to the sound barrier.  If it's possible, we're probably just overfitting to the data.</p>
<p align="justify">Daniel Lemire said this game got old around 2000, but I'm not sure I agree with that exactly.  I think there is some value in finding this barrier.  Knowing just how far we can get with accuracy using collaborative and content information is useful in telling us something about the value of ratings.  However, we pretty much already <em>know</em> this, so going further will only tell us the degree of the problem, not its existence.  So good science?  My conclusion:  no.  Fun science:  maybe.  Good engineering: yes.</p>

<h3>References</h3>
<p align="justify"> J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. 2004. <em>Evaluating collaborative filtering recommender systems</em>. ACM Trans. Inf. Syst. 22, 1 (Jan. 2004), 5-53. [<a href="http://web.engr.oregonstate.edu/~herlock/papers/eval_tois.pdf" target="_blank">pdf</a>]</p>
